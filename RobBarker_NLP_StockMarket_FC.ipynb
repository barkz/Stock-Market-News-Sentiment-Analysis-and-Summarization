{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "EvCcfwuSU-fz",
   "metadata": {
    "id": "EvCcfwuSU-fz"
   },
   "source": [
    "## **Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6QR_RHvIVHT2",
   "metadata": {
    "id": "6QR_RHvIVHT2"
   },
   "source": [
    "### Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pl3dmH-EnJGl",
   "metadata": {
    "id": "pl3dmH-EnJGl"
   },
   "source": [
    "The prices of the stocks of companies listed under a global exchange are influenced by a variety of factors, with the company's financial performance, innovations and collaborations, and market sentiment being factors that play a significant role. News and media reports can rapidly affect investor perceptions and, consequently, stock prices in the highly competitive financial industry. With the sheer volume of news and opinions from a wide variety of sources, investors and financial analysts often struggle to stay updated and accurately interpret its impact on the market. As a result, investment firms need sophisticated tools to analyze market sentiment and integrate this information into their investment strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vn6bbxSwVKl3",
   "metadata": {
    "id": "Vn6bbxSwVKl3"
   },
   "source": [
    "### Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jCIswL3zobj6",
   "metadata": {
    "id": "jCIswL3zobj6"
   },
   "source": [
    "With an ever-rising number of news articles and opinions, an investment startup aims to leverage artificial intelligence to address the challenge of interpreting stock-related news and its impact on stock prices. They have collected historical daily news for a specific company listed under NASDAQ, along with data on its daily stock price and trade volumes.\n",
    "\n",
    "As a member of the Data Science and AI team in the startup, you have been tasked with analyzing the data, developing an AI-driven sentiment analysis system that will automatically process and analyze news articles to gauge market sentiment, and summarizing the news at a weekly level to enhance the accuracy of their stock price predictions and optimize investment strategies. This will empower their financial analysts with actionable insights, leading to more informed investment decisions and improved client outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZJOtDHVSF5hu",
   "metadata": {
    "id": "ZJOtDHVSF5hu"
   },
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZlkjI8V5F9RK",
   "metadata": {
    "id": "ZlkjI8V5F9RK"
   },
   "source": [
    "* `Date` : The date the news was released\n",
    "* `News` : The content of news articles that could potentially affect the company's stock price\n",
    "* `Open` : The stock price (in \\$) at the beginning of the day\n",
    "* `High` : The highest stock price (in \\$) reached during the day\n",
    "* `Low` :  The lowest stock price (in \\$) reached during the day\n",
    "* `Close` : The adjusted stock price (in \\$) at the end of the day\n",
    "* `Volume` : The number of shares traded during the day\n",
    "* `Label` : The sentiment polarity of the news content\n",
    "    * 1: positive\n",
    "    * 0: neutral\n",
    "    * -1: negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47637cd3",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# **Setup Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da145a",
   "metadata": {},
   "source": [
    "## Development Environment\n",
    "* Local development with Visual Studio Code.\n",
    "* Jupyter Notebook and Python 3.11.7 with Anaconda3. \n",
    "* Google Colab/Drive not used.\n",
    "* Generated HTML using the jupyter cli\n",
    "\n",
    "   ```jupyter nbconvert --to html RobBarker_CV_PlantSeeding_FC.html.ipynb```\n",
    "\n",
    "* Added --- (markdown) lines for easier readability for myself. \n",
    "\n",
    "## Formatting Notes\n",
    "* Moved helper functions into separate sections according to task.\n",
    "* Added line separators for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VrFQHcW5mYgv",
   "metadata": {
    "id": "VrFQHcW5mYgv"
   },
   "source": [
    "## **Installing and Importing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "A-E2-iaumpo8",
   "metadata": {
    "id": "A-E2-iaumpo8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (8.1.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "1.2.2\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (1.2.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.local/lib/python3.11/site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "Successfully installed scikit-learn-1.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Installing the libraries with the specified version.\n",
    "# Uncomment and run the following line if Google Colab is being used\n",
    "#!pip install -U sentence-transformers gensim transformers tqdm -q\n",
    "\n",
    "# Install the required libraries\n",
    "%pip install -U sentence-transformers gensim transformers tqdm -q\n",
    "\n",
    "# Upgrade the click package to resolve the dependency conflict\n",
    "%pip install click --upgrade\n",
    "\n",
    "# Check the version of scikit-learn\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# Reinstall scikit-learn if necessary\n",
    "%pip install --upgrade scikit-learn\n",
    "\n",
    "# To manipulate and analyze data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# To visualize data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To used time-related functions\n",
    "import time\n",
    "\n",
    "# To parse JSON data\n",
    "import json\n",
    "\n",
    "# To build, tune, and evaluate ML models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# To load/create word embeddings\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# To work with transformer models\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# To implement progress bar related functionalities\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# To suppress warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wQ46zPgumfjF",
   "metadata": {
    "id": "wQ46zPgumfjF"
   },
   "source": [
    "## **Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "CrlkyDb9yNhn",
   "metadata": {
    "id": "CrlkyDb9yNhn"
   },
   "outputs": [],
   "source": [
    "# Load dataset.\n",
    "stocknews_df_org = pd.read_csv(\"/Users/barkz/Desktop/GL Projects/Stock-Market-News-Sentiment-Analysis-and-Summarization/stock_news.csv\")\n",
    "stocknews_df = stocknews_df_org.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EvFNfrvGWthn",
   "metadata": {
    "id": "EvFNfrvGWthn"
   },
   "source": [
    "## **Data Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbH_NB6oyQKT",
   "metadata": {
    "id": "bbH_NB6oyQKT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hGHBK8-QeKOB",
   "metadata": {
    "id": "hGHBK8-QeKOB"
   },
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q0UlMQnyegl7",
   "metadata": {
    "id": "Q0UlMQnyegl7"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9GVt_AAbe29X",
   "metadata": {
    "id": "9GVt_AAbe29X"
   },
   "source": [
    "* Distribution of individual variables\n",
    "* Compute and check the distribution of the length of news content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hLE0s7OFKilB",
   "metadata": {
    "id": "hLE0s7OFKilB"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yn_9wfzxL-r1",
   "metadata": {
    "id": "Yn_9wfzxL-r1"
   },
   "source": [
    "* Correlation\n",
    "* Sentiment Polarity vs Price\n",
    "* Date vs Price\n",
    "\n",
    "**Note**: The above points are listed to provide guidance on how to approach bivariate analysis. Analysis has to be done beyond the above listed points to get maximum scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N8z4-vOBmwqv",
   "metadata": {
    "id": "N8z4-vOBmwqv"
   },
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3nutDrxdzDee",
   "metadata": {
    "id": "3nutDrxdzDee"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0rYgR14ORf7b",
   "metadata": {
    "id": "0rYgR14ORf7b"
   },
   "source": [
    "## **Word Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H1_lN_o0zKlW",
   "metadata": {
    "id": "H1_lN_o0zKlW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4lwYN5bYmHp",
   "metadata": {
    "id": "f4lwYN5bYmHp"
   },
   "source": [
    "## **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hvn8PxDmz063",
   "metadata": {
    "id": "Hvn8PxDmz063"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0qP5KTLo3OOC",
   "metadata": {
    "id": "0qP5KTLo3OOC"
   },
   "source": [
    "## **Weekly News Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UX5laeu27ZEt",
   "metadata": {
    "id": "UX5laeu27ZEt"
   },
   "source": [
    "**Important Note**: It is recommended to run this section of the project independently from the previous sections in order to avoid runtime crashes due to RAM overload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oWvf3R3An5K4",
   "metadata": {
    "id": "oWvf3R3An5K4"
   },
   "source": [
    "#### Installing and Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7CZm5cGb3Qnq",
   "metadata": {
    "id": "7CZm5cGb3Qnq"
   },
   "outputs": [],
   "source": [
    "# Installation for GPU llama-cpp-python\n",
    "# uncomment and run the following code in case GPU is being used\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python -q\n",
    "\n",
    "# Installation for CPU llama-cpp-python\n",
    "# uncomment and run the following code in case GPU is not being used\n",
    "#!CMAKE_ARGS=\"-DLLAMA_CUBLAS=off\" FORCE_CMAKE=1 pip install llama-cpp-python -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KcRnKYZnCulX",
   "metadata": {
    "id": "KcRnKYZnCulX"
   },
   "outputs": [],
   "source": [
    "# Function to download the model from the Hugging Face model hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Importing the Llama class from the llama_cpp module\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Importing the library for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm # For progress bar related functionalities\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tZHRX2P2f40H",
   "metadata": {
    "id": "tZHRX2P2f40H"
   },
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i3mS-CYPz4ll",
   "metadata": {
    "id": "i3mS-CYPz4ll"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "q5ACqb0C7lxn",
   "metadata": {
    "id": "q5ACqb0C7lxn"
   },
   "source": [
    "#### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MtnDYTc80Pj3",
   "metadata": {
    "id": "MtnDYTc80Pj3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ueZl6x_0m6dJ",
   "metadata": {
    "id": "ueZl6x_0m6dJ"
   },
   "source": [
    "#### Aggregating the data weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x2_Wb4rTz_cI",
   "metadata": {
    "id": "x2_Wb4rTz_cI"
   },
   "outputs": [],
   "source": [
    "data[\"Date\"] = pd.to_datetime(data['Date'])  # Convert the 'Date' column to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AmaZ-8wG0GKS",
   "metadata": {
    "id": "AmaZ-8wG0GKS"
   },
   "outputs": [],
   "source": [
    "# Group the data by week using the 'Date' column.\n",
    "weekly_grouped = data.groupby(pd.Grouper(key='Date', freq='W'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8O9L9hZy0mto",
   "metadata": {
    "id": "8O9L9hZy0mto"
   },
   "outputs": [],
   "source": [
    "weekly_grouped = weekly_grouped.agg(\n",
    "    {\n",
    "        'News': lambda x: ' || '.join(x)  # Join the news values with ' || ' separator.\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "print(weekly_grouped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WtjfOgZ41bW7",
   "metadata": {
    "id": "WtjfOgZ41bW7"
   },
   "outputs": [],
   "source": [
    "weekly_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R31ZwDpRbcwh",
   "metadata": {
    "id": "R31ZwDpRbcwh"
   },
   "outputs": [],
   "source": [
    "# creating a copy of the data\n",
    "data_1 = weekly_grouped.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V-mESljUbcwY",
   "metadata": {
    "id": "V-mESljUbcwY"
   },
   "source": [
    "#### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AuoEISFH59m0",
   "metadata": {
    "id": "AuoEISFH59m0"
   },
   "source": [
    "**Note**:\n",
    "\n",
    "- The model is expected to summarize the news from the week by identifying the top three positive and negative events that are most likely to impact the price of the stock.\n",
    "\n",
    "- As an output, the model is expected to return a JSON containing two keys, one for Positive Events and one for Negative Events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ottdZvWzWWY9",
   "metadata": {
    "id": "ottdZvWzWWY9"
   },
   "source": [
    "For the project, we need to define the prompt to be fed to the LLM to help it understand the task to perform. The following should be the components of the prompt:\n",
    "\n",
    "1. **Role**: Specifies the role the LLM will be taking up to perform the specified task, along with any specific details regarding the role\n",
    "\n",
    "  - **Example**: `You are an expert data analyst specializing in news content analysis.`\n",
    "\n",
    "2. **Task**: Specifies the task to be performed and outlines what needs to be accomplished, clearly defining the objective\n",
    "\n",
    "  - **Example**: `Analyze the provided news headline and return the main topics contained within it.`\n",
    "\n",
    "3. **Instructions**: Provides detailed guidelines on how to perform the task, which includes steps, rules, and criteria to ensure the task is executed correctly\n",
    "\n",
    "  - **Example**:\n",
    "\n",
    "```\n",
    "Instructions:\n",
    "1. Read the news headline carefully.\n",
    "2. Identify the main subjects or entities mentioned in the headline.\n",
    "3. Determine the key events or actions described in the headline.\n",
    "4. Extract relevant keywords that represent the topics.\n",
    "5. List the topics in a concise manner.\n",
    "```\n",
    "\n",
    "4. **Output Format**: Specifies the format in which the final response should be structured, ensuring consistency and clarity in the generated output\n",
    "\n",
    "  - **Example**: `Return the output in JSON format with keys as the topic number and values as the actual topic.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1C7HTeXVYn8i",
   "metadata": {
    "id": "1C7HTeXVYn8i"
   },
   "source": [
    "**Full Prompt Example**:\n",
    "\n",
    "```\n",
    "You are an expert data analyst specializing in news content analysis.\n",
    "\n",
    "Task: Analyze the provided news headline and return the main topics contained within it.\n",
    "\n",
    "Instructions:\n",
    "1. Read the news headline carefully.\n",
    "2. Identify the main subjects or entities mentioned in the headline.\n",
    "3. Determine the key events or actions described in the headline.\n",
    "4. Extract relevant keywords that represent the topics.\n",
    "5. List the topics in a concise manner.\n",
    "\n",
    "Return the output in JSON format with keys as the topic number and values as the actual topic.\n",
    "```\n",
    "\n",
    "**Sample Output**:\n",
    "\n",
    "`{\"1\": \"Politics\", \"2\": \"Economy\", \"3\": \"Health\" }`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "re3HZr3BcB5v",
   "metadata": {
    "id": "re3HZr3BcB5v"
   },
   "source": [
    "##### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3Zy4GD2DcF9z",
   "metadata": {
    "id": "3Zy4GD2DcF9z"
   },
   "outputs": [],
   "source": [
    "# defining a function to parse the JSON output from the model\n",
    "def extract_json_data(json_str):\n",
    "    import json\n",
    "    try:\n",
    "        # Find the indices of the opening and closing curly braces\n",
    "        json_start = json_str.find('{')\n",
    "        json_end = json_str.rfind('}')\n",
    "\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            extracted_category = json_str[json_start:json_end + 1]  # Extract the JSON object\n",
    "            data_dict = json.loads(extracted_category)\n",
    "            return data_dict\n",
    "        else:\n",
    "            print(f\"Warning: JSON object not found in response: {json_str}\")\n",
    "            return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XwtB-hG4h9GL",
   "metadata": {
    "id": "XwtB-hG4h9GL"
   },
   "source": [
    "##### Defining the response function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wNXpBE7sDrr-",
   "metadata": {
    "id": "wNXpBE7sDrr-"
   },
   "outputs": [],
   "source": [
    "#Defining the response function\n",
    "def response_mistral_1(prompt, news):\n",
    "    model_output = llm(\n",
    "      f\"\"\"\n",
    "      [INST]\n",
    "      {prompt}\n",
    "      News Articles: {news}\n",
    "      [/INST]\n",
    "      \"\"\",\n",
    "      max_tokens=_____, #Complete the code to set the maximum number of tokens the model should generate for this task.\n",
    "      temperature=_____, #Complete the code to set the value for temperature.\n",
    "      top_p=_____, #Complete the code to set the value for top_p\n",
    "      top_k=_____, #Complete the code to set the value for top_k\n",
    "      echo=False,\n",
    "    )\n",
    "\n",
    "    final_output = model_output[\"choices\"][0][\"text\"]\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JRsgiVSXbcwi",
   "metadata": {
    "id": "JRsgiVSXbcwi"
   },
   "source": [
    "##### Checking the model output on a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "osPo6gq_0vZa",
   "metadata": {
    "id": "osPo6gq_0vZa"
   },
   "source": [
    "**Note**: Use this section to test out the prompt with one instance before using it for the entire weekly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "erNvfAZs0o7B",
   "metadata": {
    "id": "erNvfAZs0o7B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "nFNa_MMwTCyA",
   "metadata": {
    "id": "nFNa_MMwTCyA"
   },
   "source": [
    "##### Checking the model output on the weekly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pSYYfSCb0pYw",
   "metadata": {
    "id": "pSYYfSCb0pYw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "C-MW3XpmQfqN",
   "metadata": {
    "id": "C-MW3XpmQfqN"
   },
   "source": [
    "##### Formatting the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cs3Ww3J0py_",
   "metadata": {
    "id": "2cs3Ww3J0py_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "HiOLoD7BO3L-",
   "metadata": {
    "id": "HiOLoD7BO3L-"
   },
   "source": [
    "## **Conclusions and Recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5r0z8qUZ4MOE",
   "metadata": {
    "id": "5r0z8qUZ4MOE"
   },
   "source": [
    "-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uAOPv9oQtWcC",
   "metadata": {
    "id": "uAOPv9oQtWcC"
   },
   "source": [
    "<font size=6 color='blue'>Power Ahead</font>\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "EvCcfwuSU-fz",
    "6QR_RHvIVHT2",
    "Vn6bbxSwVKl3",
    "ZJOtDHVSF5hu",
    "EWGlpDNkrTqA",
    "VrFQHcW5mYgv",
    "wQ46zPgumfjF",
    "EvFNfrvGWthn",
    "hGHBK8-QeKOB",
    "Q0UlMQnyegl7",
    "hLE0s7OFKilB",
    "N8z4-vOBmwqv",
    "0rYgR14ORf7b",
    "f4lwYN5bYmHp",
    "0qP5KTLo3OOC",
    "oWvf3R3An5K4",
    "tZHRX2P2f40H",
    "q5ACqb0C7lxn",
    "ueZl6x_0m6dJ",
    "V-mESljUbcwY",
    "re3HZr3BcB5v",
    "XwtB-hG4h9GL",
    "JRsgiVSXbcwi",
    "nFNa_MMwTCyA",
    "C-MW3XpmQfqN",
    "HiOLoD7BO3L-"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
